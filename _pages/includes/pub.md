
# üìù Publications 



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/roboground.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[RoboGround: Robotic Manipulation with Grounded Vision-Language Priors.](https://openaccess.thecvf.com/content/CVPR2025/html/Huang_RoboGround_Robotic_Manipulation_with_Grounded_Vision-Language_Priors_CVPR_2025_paper.html) [![](https://img.shields.io/github/stars/ZzZZCHS/RoboGround?style=social&label=Code+Stars)](https://github.com/ZzZZCHS/RoboGround)

**Haifeng Huang**, Xinyi Chen, Yilun Chen, Hao Li, Xiaoshen Han, Zehan Wang, Tai Wang, Jiangmiao Pang, Zhou Zhao

- Create a large-scale simulated robotic manipulation dataset with a diverse set of objects and instructions (24K demonstrations, 112K instructions, and 3,526 unique objects from 176 categories).
- Develop a grounding-aware robotic manipulation policy that leverages grounding masks as an intermediate representation to guide policy networks in object manipulation tasks.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/chat-scene.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers.](https://arxiv.org/abs/2312.08168) [![](https://img.shields.io/github/stars/ZzZZCHS/Chat-Scene?style=social&label=Code+Stars)](https://github.com/ZzZZCHS/Chat-Scene)

**Haifeng Huang**, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, Zhou Zhao

- Chat-Scene is a 3D LLM which processes both point clouds and multi-view images for 3D scene understanding, excelling in tasks such as 3D grounding, captioning, and question answering.
- (Sep. 2024) Ranked 1st on the grounding benchmark [ScanRefer](https://kaldir.vc.in.tum.de/scanrefer_benchmark/benchmark_localization) and the captioning benchmark [Scan2Cap](https://kaldir.vc.in.tum.de/scanrefer_benchmark/benchmark_captioning).

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2024</div><img src='images/grounded-3dllm.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Grounded 3D-LLM with Referent Tokens.](https://groundedscenellm.github.io/grounded_3d-llm.github.io/) [![](https://img.shields.io/github/stars/OpenRobotLab/Grounded_3D-LLM?style=social&label=Code+Stars)](https://github.com/OpenRobotLab/Grounded_3D-LLM)

Yilun Chen\*, Shuai Yang\*, **Haifeng Huang\***, Tai Wang, Ruiyuan Lyu, Runsen Xu, Dahua Lin, Jiangmiao Pang.

- Grounded 3D-LLM establishes a correspondence between 3D scenes and language phrases through referent tokens.
- Create a large-scale grounded scene caption dataset at phrase-level.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2023</div><img src='images/chat-3d.png' alt="sym" width="50%"></div></div>
<div class='paper-box-text' markdown="1">

[Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes.](https://chat-3d.github.io/) [![](https://img.shields.io/github/stars/Chat-3D/Chat-3D?style=social&label=Code+Stars)](https://github.com/Chat-3D/Chat-3D)

Zehan Wang\*, **Haifeng Huang\***, Yang Zhao, Ziang Zhang, Zhou Zhao.

- Chat-3D is one of the frist 3D LLMs.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/ws-3dvg.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding.](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Distilling_Coarse-to-Fine_Semantic_Matching_Knowledge_for_Weakly_Supervised_3D_Visual_ICCV_2023_paper.html)

Zehan Wang\*, **Haifeng Huang\***, Yang Zhao, Linjun Li, Xize Cheng, Yichen Zhu, Aoxiong Yin, Zhou Zhao

- The first weakly-supervised 3D visual grounding method.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/turn.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Towards Effective Multi-modal Interchanges in Zero-resource Sounding Object Localization.](https://proceedings.neurips.cc/paper_files/paper/2022/hash/f8de10c9ff056ae3d1eef43ad1762351-Abstract-Conference.html)

Yang Zhao\*, Chen Zhang\*, **Haifeng Huang\***, Haoyuan Li, Zhou Zhao

- A method for sounding object localization without training on any prior data in this field.

</div>
</div>